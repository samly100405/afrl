{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidating data\n",
    "\n",
    "This file consists of functions that consolidate our disparate datasets into one large dataset that is useful in training our model. \n",
    "\n",
    "The goal is to generate a file with 30 columns (this number should be variable), such that each column is a state in time. \n",
    "\n",
    "Ideally, this will be done with heirachical data, ie `p1` is the first point in time, and within `p1` you have an x component, y component, etc.\n",
    "\n",
    "https://pandas.pydata.org/docs/user_guide/advanced.html\n",
    "\n",
    "## Input data format\n",
    "\n",
    "It is assumed that the input data with have the columns: `[timestamp,tx,ty,tz,qx,qy,qz,qw]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data we want\n",
    "\n",
    "This function will create velocity and acceleration columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def extract_features(raw: pd.DataFrame, dropna: bool = False) -> None:\n",
    "    raw['vx'] = raw['tx'].diff() / raw['timestamp'].diff()\n",
    "    raw['vy'] = raw['ty'].diff() / raw['timestamp'].diff()\n",
    "    raw['vz'] = raw['tz'].diff() / raw['timestamp'].diff()\n",
    "\n",
    "    raw['ax'] = raw['vx'].diff() / raw['timestamp'].diff()\n",
    "    raw['ay'] = raw['vy'].diff() / raw['timestamp'].diff()\n",
    "    raw['az'] = raw['vz'].diff() / raw['timestamp'].diff()\n",
    "\n",
    "    if dropna: raw.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the above functions\n",
    "\n",
    "df = pd.read_csv(\"../data/fpv_uzh/indoor_forward_3_davis_with_gt.txt\")\n",
    "\n",
    "extract_features(df, dropna=True)\n",
    "\n",
    "print(df.head())\n",
    "print(df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing the data\n",
    "\n",
    "Now, we want rows of data that represent a specific range of time. In this case, we want 30 points for each new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slices(data: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    # each row in the original data is a \"point\". Each row in the output \n",
    "    # is a list of points of size n. \n",
    "    indices = [[f\"p{i}\" for i in range(n)], data.columns]\n",
    "    m_index = pd.MultiIndex.from_product(indices, names=[\"points\", \"data\"])\n",
    "    slices = []\n",
    "    for i in range(len(data) - n):\n",
    "        flattened = pd.DataFrame([data[i:i+n].to_numpy().flatten()], index=m_index, columns=data.columns)\n",
    "        print('flattened', flattened.columns)\n",
    "        slices.append(flattened)\n",
    "        \n",
    "        # redo this later\n",
    "        # for k in range(n):\n",
    "        #     out[f\"p{k}\"] = slice[k]\n",
    "        \n",
    "    return pd.concat(slices, ignore_index=False)\n",
    "\n",
    "# test the above function\n",
    "\n",
    "slices = generate_slices(df, 4)\n",
    "print(slices.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate all our original data\n",
    "\n",
    "Now, we want to consolidate our data from all the other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "fpv_data = \"../data/fpv_uzh\"\n",
    "random_traj_data = \"../data/random_trajectory_100ms\"\n",
    "output_path = \"../data/output\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "n = 30 # we want 30 points per row\n",
    "slices = []\n",
    "\n",
    "# consolidate the fpv data\n",
    "for filename in filter(lambda p: p.endswith(\"txt\"), os.listdir(fpv_data)):\n",
    "    filepath = os.path.join(fpv_data, filename)\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    extract_features(df)\n",
    "    # the columns should be handled in the generate slices funciton\n",
    "    slices.append(generate_slices(df, n))\n",
    "    \n",
    "# consolidate the synthetic data\n",
    "for filename in filter(lambda p: p.endswith(\"txt\"), os.listdir(random_traj_data)):\n",
    "    filepath = os.path.join(random_traj_data, filename)\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    extract_features(df)\n",
    "    # the columns should be handled in the generate slices funciton\n",
    "    slices.append(generate_slices(df, n))\n",
    "\n",
    "consolidated = pd.concat(slices, ignore_index=False)\n",
    "consolidated.to_csv(os.path.join(output_path, \"consolidated.csv\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
