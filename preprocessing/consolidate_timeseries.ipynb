{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidating data\n",
    "\n",
    "This file consists of functions that consolidate our disparate datasets into one large dataset that is useful in training our model. \n",
    "\n",
    "The goal is to generate a file with 30 columns (this number should be variable), such that each column is a state in time. \n",
    "\n",
    "Ideally, this will be done with heirachical data, ie `p1` is the first point in time, and within `p1` you have an x component, y component, etc.\n",
    "\n",
    "https://pandas.pydata.org/docs/user_guide/advanced.html\n",
    "\n",
    "## Input data format\n",
    "\n",
    "It is assumed that the input data with have the columns: `[timestamp,tx,ty,tz,qx,qy,qz,qw]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data we want\n",
    "\n",
    "In our case, we want just the velocity data (for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_velocity(position_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    velocity_data = {\n",
    "        'timestamp': position_data['timestamp'],\n",
    "        'vx': position_data['tx'].diff() / position_data['timestamp'].diff(),\n",
    "        'vy': position_data['ty'].diff() / position_data['timestamp'].diff(),\n",
    "        'vz': position_data['tz'].diff() / position_data['timestamp'].diff()\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(velocity_data).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing the data\n",
    "\n",
    "Now, we want rows of data that represent a specific range of time. In this case, we want 30 points for each new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slices(data: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    # each row in the original data is a \"point\". Each row in the output \n",
    "    # is a list of points of size n. \n",
    "    cols = [f\"{col}_{i}\" for i in range(n) for col in data.columns]\n",
    "    out = pd.DataFrame(columns=cols)\n",
    "    for i in range(len(data) - n):\n",
    "        flattened = pd.DataFrame([data[i:i+n].to_numpy().flatten()])\n",
    "        flattened.columns = cols\n",
    "        out = pd.concat([out if not out.empty else None, flattened], ignore_index=False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate all our original data\n",
    "\n",
    "Now, we want to consolidate our data from all the other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "fpv_data = \"../data/fpv_uzh\"\n",
    "random_traj_data = \"../data/random_trajectory_100ms\"\n",
    "output_path = \"../data/output\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "n = 30 # we want 30 points per row\n",
    "slices = []\n",
    "\n",
    "# consolidate the fpv data\n",
    "for filename in filter(lambda p: p.endswith(\"txt\"), os.listdir(fpv_data)):\n",
    "    filepath = os.path.join(fpv_data, filename)\n",
    "    pos_df = pd.read_csv(filepath)\n",
    "    \n",
    "    vel_df = generate_velocity(pos_df)\n",
    "    # the columns should be handled in the generate slices funciton\n",
    "    slices.append(generate_slices(vel_df, n))\n",
    "    \n",
    "# consolidate the synthetic data\n",
    "for filename in filter(lambda p: p.endswith(\"txt\"), os.listdir(random_traj_data)):\n",
    "    filepath = os.path.join(random_traj_data, filename)\n",
    "    pos_df = pd.read_csv(filepath)\n",
    "    \n",
    "    vel_df = generate_velocity(pos_df)\n",
    "    # the columns should be handled in the generate slices funciton\n",
    "    slices.append(generate_slices(vel_df, n))\n",
    "\n",
    "consolidated = pd.concat(slices, ignore_index=False)\n",
    "consolidated.to_csv(os.path.join(output_path, \"consolidated.csv\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
